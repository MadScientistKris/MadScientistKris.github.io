<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Classification on Yingxi Yu</title>
    <link>/tags/classification/index.xml</link>
    <description>Recent content in Classification on Yingxi Yu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy;2017 Yingxi Yu, credit to vincentz</copyright>
    <atom:link href="/tags/classification/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>SPAM FILTER</title>
      <link>/projects/spam_detection/</link>
      <pubDate>Wed, 01 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/projects/spam_detection/</guid>
      <description>&lt;p&gt;You got a spam email! Gmail can always help you filtering spam emails. Do you want to know how it works?
&lt;/p&gt;

&lt;h3&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Email detection is one of the most importand applications of Natural Language Processing. The common ways are Hand-coded rules which is quite complicated in processing with low recall rate and supervised learning which includes Naïve Bayes、Logistic regression、Support-vector machines、k-Nearest Neighbors, etc. Here I will use the Naïve Bayes Classifier since it is more straightforward and simpler than other algorithms.&lt;!--more--&gt;&lt;/p&gt;

&lt;h3&gt;Algorithm Details&lt;/h3&gt;
Naïve Bayes is based on bayes rule and bag of words analysis with a very important assumption: tokens appeard in the text are disorderly and independant with each other. The Naïve Bayes Classifier can be explained like this:
$$P(c|d) = \frac{P(d|c)P(c)}{P(d)}$$ where d represents document and c represents classes. The class c of the given document d is:
$$
\begin{split}
C &amp;= argmax\ P(c|d) \\
&amp;=argmax\ \frac{P(d|c)P(c)}{P(d)}\\
&amp;=argmax\ P(d|c)P(c)\\
&amp;=argmax\ P(t_1,t_2,...,t_n|c)P(c)\\
&amp;=argmax\ P(t_1|c)P(t_2|c),...,P(t_n|c)P(c) \\
&amp;=argmax\ logP(t_1|c)+logP(t_2|c)+,...,+logP(t_n|c)+logP(c)
\end{split}
$$
where \(t_1,...,t_n\) are tokens in the given text. \(P(c)\) is the prior probability, \(P(t_1,t_2,...,t_n|c\) is the conditional probability, namely likelihood and P(c|d) is the posterior probability.



&lt;h3&gt;Training Phase&lt;/h3&gt;

&lt;p&gt;Our training data are emails in the SPAM_trainig folder with the file name like &amp;lsquo;HAM.02281.txt&amp;rsquo; representing ham email and &amp;lsquo;SPAM.02281.txt&amp;rsquo; representing spam email.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import nltk
import pickle
import math
import re
from collections import Counter
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1. Load our training files and take a look at the contents&lt;/h4&gt;

&lt;p&gt;ham stores all true emails with the file name like HAM.0123.txt while spam stores all spam emails with the file name like SPAM.0234.txt&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ham = nltk.corpus.PlaintextCorpusReader(&#39;/Users/Aslan/winter 2017/Lin127/hw2,3/SPAM_training&#39;, &#39;HAM.*.txt&#39;)
spam = nltk.corpus.PlaintextCorpusReader(&#39;/Users/Aslan/winter 2017/Lin127/hw2,3/SPAM_training&#39;, &#39;SPAM.*.txt&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are 13545 ham emails and 4912 spam emails in our training dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# number of files in each class
ham_count = len(ham.fileids())
spam_count = len(spam.fileids())
# total number of tokens in each class
ham_tokens = ham.words()
spam_tokens = spam.words()
print(ham_count, spam_count)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;13545 4912
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;2. Build our text classifier&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_freq_df(words):
    &#39;&#39;&#39;Build a nltk frequency table for the token types in each class
    
    Args:
        words(nltk object): a list of tokens
    
    Returns:
        dist_words(nltk FreqDist): frequency table
    &#39;&#39;&#39;
    dist_words = nltk.FreqDist(words)
    return dist_words
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def solver(spam, ham, spam_count, ham_count):
    &#39;&#39;&#39;build the solver model
    Args: 
        spam(nltk object): a list of tokens in spam
        ham(nltk object): a list of tokens in ham
    Returns:
        a dict with two data frames and two counts for each class 
    &#39;&#39;&#39;
    df_spam = get_freq_df(spam)
    df_ham = get_freq_df(ham)

    return {&#39;spam_fd&#39;: df_spam, &#39;ham_fd&#39;: df_ham, 
            &#39;spam_count&#39;: spam_count, &#39;ham_count&#39;: ham_count}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3. Save our model into spam.nb for time saving when reusing this code.&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;solver_NB = solver(spam_tokens, ham_tokens, spam_count, ham_count)
with open(&#39;spam.nb&#39;, &#39;wb&#39;) as f:
    pickle.dump(solver_NB, f, protocol=pickle.HIGHEST_PROTOCOL)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Testing Phase&lt;/h3&gt;

&lt;h4&gt;1. we reload our classifier model from our hard disk&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with open(&#39;spam.nb&#39;, &#39;rb&#39;) as f:
    model = pickle.load(f)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;log_prior_spam/ham represents the prior logrithm probability of spam/ham class.
we can see that there are, in total,  1284301 tokens in spam training data and 4903935 in ham data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;log_prior_spam = math.log(model[&#39;spam_count&#39;]/(model[&#39;spam_count&#39;] + model[&#39;ham_count&#39;]))
log_prior_ham = math.log(model[&#39;ham_count&#39;]/(model[&#39;spam_count&#39;] + model[&#39;ham_count&#39;]))
spam_n = model[&#39;spam_fd&#39;].N()
ham_n = model[&#39;ham_fd&#39;].N()
print(spam_n, ham_n)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;1284301 4903935
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;2. create the likelihood table with add-one-smoothing modification&lt;/h4&gt;

&lt;p&gt;The add-one-smoothing is one of the methods to fix the new word problem. When there are new words which can be found only from one of our training class(spam or ham) in the text files, the posterior probablity could be zero since \(P(new|c) = 0\) for the class that does not include that new word. We don&amp;rsquo;t want our classifier be disturbed by these new words and that is why we should implement this modification here.&lt;/p&gt;

&lt;p&gt;To be more detailed:
$$P(token|c) = \frac{(number\ of\ the\ given\ token\ in\ class\ c) +1}{(token\ size\ of\ class\ c) + (vocabulary\ size)}$$ where the vocabulary size is the total number of token type in the union of spam and ham training set&lt;/p&gt;

&lt;p&gt;However, add-one-smoothing can&amp;rsquo;t fix the scenario where the new word doesn&amp;rsquo;t show in any traning class. In that case, I just simply omit that word.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Build the vacabulary set on the union of spam and ham training set.
vocabulary = set(list(model[&#39;ham_fd&#39;].keys()) + list(model[&#39;spam_fd&#39;].keys()))
vocal_size = len(vocabulary)
vocal_size
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;101357
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;spam_log_likelihood = {}    # initialize the conditional probablity
ham_log_likelihood = {}
# add-one-smoothing
for token in vocabulary:
    spam_log_likelihood[token] = math.log((model[&#39;spam_fd&#39;][token] + 1) / (spam_n + vocal_size))
    ham_log_likelihood[token] = math.log((model[&#39;ham_fd&#39;][token] + 1) / (ham_n + vocal_size))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def spam_detection(file, ham_log_likelihood, spam_log_likelihood, 
                   log_prior_spam, log_prior_ham, vocabulary):
    &#39;&#39;&#39; classify the given file as a binary outcome(spam or ham)
    Args:
        file: file name(eg.&#39;test.0123.txt&#39;)
        ham(spam)_log_likelihood: likelihood table after taking add-one-smoothing and logrithm
        log_prior_spam(ham): prior probability in log type
        vocabulary: token types in the union of spam and ham training dataset.
    Returns:
        print file name with &#39;SPAM&#39; or &#39;HAM&#39;
    &#39;&#39;&#39;       
    token_ls = dev.words(fileids=file)
    log_posterior_spam = log_prior_spam
    log_posterior_ham = log_prior_ham
    
    for token in token_ls:
        if not token in vocabulary:
            continue
        log_posterior_spam += spam_log_likelihood[token]
        log_posterior_ham += ham_log_likelihood[token]
    
    if log_posterior_spam &amp;gt; log_posterior_ham:
        print(file, &#39;SPAM&#39;)
    else:
        print(file, &#39;HAM&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3. Run the classifier on our test data&lt;/h4&gt;

&lt;p&gt;I wrote the code above in a more decent way in a script file called nbtest.py and run the shell command:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;python3 nbtest.py &amp;gt; SPAM_dev_predictions.txt&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Then we load the result into variable dev by nltk&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dev = nltk.corpus.PlaintextCorpusReader(&#39;/Users/Aslan/winter 2017/Lin127/hw2,3/&#39;, &#39;SPAM_dev_predictions.txt&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Model Evaluation Phase&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;file_ls = dev.raw().strip().split(&#39;\n&#39;)
confusion = [re.search(r&#39;(HAM|SPAM)\..*(HAM|SPAM)&#39;, i).group(1) + &#39; &#39; +
             re.search(r&#39;(HAM|SPAM)\..*(HAM|SPAM)&#39;, i).group(2)
             for i in file_ls]
file_ls[:10]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[&#39;HAM.00039.txt HAM&#39;,
 &#39;HAM.00045.txt HAM&#39;,
 &#39;HAM.00064.txt HAM&#39;,
 &#39;HAM.00091.txt HAM&#39;,
 &#39;HAM.00098.txt HAM&#39;,
 &#39;HAM.00137.txt HAM&#39;,
 &#39;HAM.00143.txt HAM&#39;,
 &#39;HAM.00146.txt HAM&#39;,
 &#39;HAM.00153.txt HAM&#39;,
 &#39;HAM.00205.txt HAM&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Counter(confusion)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Counter({&#39;HAM HAM&#39;: 986, &#39;HAM SPAM&#39;: 14, &#39;SPAM HAM&#39;: 14, &#39;SPAM SPAM&#39;: 349})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our test set has 1000 ham and 363 spam
Therefore:&lt;/p&gt;

&lt;p&gt;\(recall(HAM) = \frac{986}{1000} = 98.6\% \)&lt;/p&gt;

&lt;p&gt;\(recall(SPAM) = \frac{349}{363} = 96.1\% \)&lt;/p&gt;

&lt;p&gt;\(precision(HAM) = \frac{986}{986+14} = 98.6\% \)&lt;/p&gt;

&lt;p&gt;\(precision(SPAM) = \frac{349}{349+14} = 96.1\% \)&lt;/p&gt;

&lt;p&gt;where \(t_1,&amp;hellip;,t_n\) are tokens in the given text. P(c) is the prior probability, \(P(t_1,t_2,&amp;hellip;,t_n|c\) is the conditional probability, namely likelihood and P(c|d) is the posterior probability.&lt;/p&gt;

&lt;p&gt;The recall and precision for both class are pretty high which indicate that my classifier is powerful enough.&lt;/p&gt;

&lt;h3&gt;Next Steps to be done&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Try other smoothing methods&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Find a way to deal with the new words problem which I omitted this time&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
  </channel>
</rss>