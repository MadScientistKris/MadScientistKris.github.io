<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Yingxi Yu</title>
    <link>/tags/machine-learning/index.xml</link>
    <description>Recent content in Machine Learning on Yingxi Yu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy;2017 Yingxi Yu, credit to vincentz</copyright>
    <atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Sentiment Analysis on Twitter</title>
      <link>/projects/sentiment/sentiment/</link>
      <pubDate>Fri, 10 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/projects/sentiment/sentiment/</guid>
      <description>&lt;p&gt;What is people&amp;rsquo;s attitude towards special topics in each piece of tweet? positive, neutral or negtive?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#abstract&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#material&#34;&gt;Material&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-source&#34;&gt;Data Source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#challenges&#34;&gt;Challenges&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#methodologies&#34;&gt;Methodologies&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#preprocessing&#34;&gt;Preprocessing&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#emoticons&#34;&gt;Emoticons&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hashtags&#34;&gt;Hashtags&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#handles&#34;&gt;Handles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#urls&#34;&gt;URLs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#elongated-words&#34;&gt;Elongated Words&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#negation&#34;&gt;Negation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#stop-words&#34;&gt;Stop Words&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#stem&#34;&gt;Stem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bag-of-words-model&#34;&gt;Bag-of-Words Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tf-idf-transformation&#34;&gt;Tf-idf Transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#annotation&#34;&gt;Annotation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#machine-learning-models&#34;&gt;Machine Learning Models&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#naive-bayes&#34;&gt;Naive Bayes&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression&#34;&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#svm&#34;&gt;SVM&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#result&#34;&gt;Result&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-and-future&#34;&gt;Summary and Future&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-small-sample-of-code&#34;&gt;A Small Sample of Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;We perform sentimental analysis (classification) on 400000 tweets from twitter. Our baseline model is multinomial naive bayes classifier. Then we try to improve the classifier not only by introducing algorithms with higher performance on large scale datasets such as logistic regression and support vector machine but also on linguistic level like n-gram, emoji analysis and annotation.&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Sentiment analysis, also called opinion mining, is the process of using the technique of natural language processing, text analysis, computational linguistics to determine the emotional tone or the attitude that a writer or a speaker express towards some entity. As millions of text are generated on the Internet everyday, the application of sentiment analysis becomes more powerful and broad. For example, social media monitoring allows us to gain an overview of the wider public opinion behind certain topics, and the sentiment analysis for customers review delivers the satisfaction metric to the company, which makes it possible for them to improve their products and service. It also has been applied widely in the field of market where it can be applied to forecast market movement based on news, blogs and social media sentiment. In our project, we combine the technique of text analysis and machine learning to perform sentiment classification on the twitter sentiment corpus.&lt;/p&gt;

&lt;h2 id=&#34;material&#34;&gt;Material&lt;/h2&gt;

&lt;h3 id=&#34;data-source&#34;&gt;Data Source&lt;/h3&gt;

&lt;p&gt;We choose &lt;a href=&#34;http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/&#34;&gt;Twitter Sentiment Analysis Dataset&lt;/a&gt; as our training and test data where the data sources are University of Michigan Sentiment Analysis competition on Kaggle and Twitter Sentiment Corpus by Niek Sanders. The reason why we use this dataset is that it contains 1,578,627 classified tweets from sentimental annotation which is huge enough for model building and hyperparameter tuning. Ibrahim Naji who is the author of the blog where we got the data has tried simple Naive Bayesian classification algorithm and the result were 75% which is a good baseline for us to compare and improve.&lt;/p&gt;

&lt;h3 id=&#34;challenges&#34;&gt;Challenges&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Tweets are always unstructured&lt;/li&gt;
&lt;li&gt;None vocabulary word such as emoji and emoticon.&lt;/li&gt;
&lt;li&gt;Lexical variation such as tmrw representing tomorrow,&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;methodologies&#34;&gt;Methodologies&lt;/h2&gt;

&lt;p&gt;As we all know, tweet has 140-character count limitations so they are different from those documentations that have no length limits. On top of that, unlike the usual text documentation, the tweet message has its own unique features, such as the prevalent usage of emoticons and elongated words (eg. goooood), which distinguishes itself from other text genres.  Hence it is important to standardize the text and remove noise information through some pre-processing steps.&lt;/p&gt;

&lt;h3 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h3&gt;

&lt;h4 id=&#34;emoticons&#34;&gt;Emoticons&lt;/h4&gt;

&lt;p&gt;Emoticons are used broadly throughout the Internet. However, the default setting of tokenization of nltk will treat each individual punctuation to be a token and therefore decompose the emoticons. So the first step is to identify the emoticons in each text and transform them to a single word. However this uniform transformation for each emoticon has such disadvantage that it ignores the ambiguity of emoticons. For example, the smiley face &amp;ldquo;:)&amp;rdquo; can have multiple meanings, either expressing happiness or just used by people to soften the tone. But this is the best solution we have come up with so far therefore we accept this consequence of ambiguity.&lt;/p&gt;

&lt;h4 id=&#34;hashtags&#34;&gt;Hashtags&lt;/h4&gt;

&lt;p&gt;A hashtag is a word or an un-spaced phrase prefixed with the hash symbol (#), which is used to indicate the topics. It is uncertain whether or not the hashtags contain sentiment information and it is also difficult to tokenize since often times phrase in a hashtag is un-spaced. Thus, we decide to uniformly replace every hashtag by a pattern &amp;ldquo;HASH_(whatever in the hashtag)&amp;rdquo;&lt;/p&gt;

&lt;h4 id=&#34;handles&#34;&gt;Handles&lt;/h4&gt;

&lt;p&gt;Handles are used to direct the twitter towards other users by writing their usernames preceded by ‘@’. No transformation is needed since usernames can be directly seperated by &amp;ldquo;@&amp;rdquo; and treated as proper nouns.&lt;/p&gt;

&lt;h4 id=&#34;urls&#34;&gt;URLs&lt;/h4&gt;

&lt;p&gt;Links are very common in the twitters for sharing assorted resources. Each link is replaced by a word &amp;ldquo;URL&amp;rdquo; and treated as a proper noun.&lt;/p&gt;

&lt;h4 id=&#34;elongated-words&#34;&gt;Elongated Words&lt;/h4&gt;

&lt;p&gt;People tend to use repeating characters in colloquial language, which delivers certain sentiment. But the problem is that some elongated words might derive from the same word but have different numbers of repeating characters. Therefore, we used regular expression to replace characters repeating more than twice as two characters.&lt;/p&gt;

&lt;h4 id=&#34;negation&#34;&gt;Negation&lt;/h4&gt;

&lt;p&gt;The occurrence of negation can completely change the sentiment of the whole sentence. There are several ways to handle negation, one of which is to suffix &amp;ldquo;_NEG&amp;rdquo; to all the words that appear after the negation words until the punctuation. In addition, we also learned that negation handling is way more complicated than that and people have done some research regrading detection of explicit negation cues and the scope of negation of these words. However, our analysis showed that &amp;ldquo;_NEG&amp;rdquo; appending can only improve the prediction accuracy by 0.5% compared to the unigram baseline model. Moreover, later experiment indicates that the combination of unigram, bigram and trigram can significantly improve the prediction performance and we assume that such N-gram model can capture the negation characteristics. Computational capabilities are also important and we discovered such modification requires changes for some default behavior of tokenizer (we use scikit-learn in python) and notably slowed down the computation. So we decided not to handle the negations.&lt;/p&gt;

&lt;h3 id=&#34;stop-words&#34;&gt;Stop Words&lt;/h3&gt;

&lt;p&gt;Stop words usually refer to the most common words in a language, such as &amp;ldquo;a&amp;rdquo;, &amp;ldquo;the&amp;rdquo; and &amp;ldquo;is&amp;rdquo; in English. Removing stop words are a common step in text processing. However, it has been demonstrated in recent years that the removal of stop words is not a necessary step and may have undesirable effect on the classifier. In fact, our preliminary experiment deprecated such action as the testing accuracy decreased by 2%.&lt;/p&gt;

&lt;h3 id=&#34;stem&#34;&gt;Stem&lt;/h3&gt;

&lt;p&gt;In linguistics, a stem is a part of a word. Different words can be derivative of the same stem. For example, the word &amp;ldquo;interest&amp;rdquo; is the stem of both &amp;ldquo;interesting&amp;rdquo; and &amp;ldquo;interested&amp;rdquo;.
Consequently, stemming refers to the procedure of replace every word by its stem. However, our considerations is that tweets are very short so we choose not to perform stemming since we want to reserve as many features as possible. Nevertheless, we do think it is a necessary step to do in the case of long documents in order to reduce the features and improve classification performance.&lt;/p&gt;

&lt;h3 id=&#34;bag-of-words-model&#34;&gt;Bag of Words Model&lt;/h3&gt;

&lt;p&gt;The bag-of-words model is a simplifying representation used in natural language processing. In this model, a text is represented as the collection of its words, disregarding grammar and even word order but keeping multiplicity. One can also consider bag of bigram and trigram tokens as it can retain some local information in the text. It is also worth pointing out it is better to use pointwise mutual information (PMI) to select those N-gram patterns that are more likely to occur together. But we abstained from doing so as it is too computational expensive.&lt;/p&gt;

&lt;h3 id=&#34;tf-idf-transformation&#34;&gt;Tf Idf Transformation&lt;/h3&gt;

&lt;p&gt;After bag-of-words tokenization, each text ends up to be a set of words with corresponding counts. The simple counts representation has such a disadvantage that the longer document tends to have larger counts for some words. In order to fix this, it is better to divide the counts by the size of the document, then the counts become term frequency (tf). Another consideration is inverse document-frequency (idf). In a large text corpus, some words will be very present (e.g. &amp;ldquo;the&amp;rdquo;, &amp;ldquo;a&amp;rdquo;, &amp;ldquo;is&amp;rdquo; in English) hence carrying very little meaningful information about the actual contents of the document. The idf is defined as $$idf(t) = \log{\frac{n_d}{1+df(d, t)}}$$, where the \(n_d\) is the total number of documents, and \(df(d,t)\) is the number of documents that contain term t and &amp;ldquo;1&amp;rdquo; is just the smoothing term. Therefore, idf will make the rarer words more informative for certain type of documents. Then the tf-idf is just the product of the two components. We decided to use idf since it had improvement over the logistic regression unigram model.&lt;/p&gt;

&lt;h3 id=&#34;annotation&#34;&gt;Annotation&lt;/h3&gt;

&lt;p&gt;There are several famous lexicons online that contain sentiment words chosen by linguists. The one we chose is called AFINN list, including 2477 words, each of which also has a sentiment score on a scale of -5 to 5 (from negative to positive). We matched each word in all the tweets message and computed the sum of sentiment score for each text. It turned out the logistic regression could achieve 61.14% prediction accuracy based on this single attribute. Afterwards, we appended attribute to the unigram bag-of-word matrix we obtained before and used logistic regression to retrain the model. The prediction accuracy on test is 81.96%, improving the logistic regression baseline slightly by 1%. However, the computation became more time consuming later on expecially for SVM and we thought the reason might be that the new attribute somehow destructed the sparsity of data matrix. Finally, we determined to ignore this attribute since it had disadvantage in computation capacity and only had small improvement.&lt;/p&gt;

&lt;h3 id=&#34;machine-learning-models&#34;&gt;Machine Learning Models&lt;/h3&gt;

&lt;h4 id=&#34;naive-bayes&#34;&gt;Naive Bayes&lt;/h4&gt;

&lt;p&gt;Naive Bayes is a generative model that makes the bag of words assumption (position doesn’t matter) and the conditional independence assumption (words are conditionally independent of each other given the class). The high bias and low variance model is a very common baseline and can do surprisingly well for small data sets.&lt;/p&gt;

&lt;h4 id=&#34;logistic-regression&#34;&gt;Logistic Regression&lt;/h4&gt;

&lt;p&gt;Logistic regression, also called maximum entropy model, is a discriminative model with good computational performance for large linearly seperated data sets.&lt;/p&gt;

&lt;h4 id=&#34;svm&#34;&gt;SVM&lt;/h4&gt;

&lt;p&gt;Support vector machines (SVMs) are supervised learning models that can have very high performance in high dimensional spaces, therefore it can be very effective in text classification.&lt;/p&gt;

&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;

&lt;p&gt;&lt;html&gt;
&lt;head&gt;
&lt;style&gt;
table, th, td {
    border: 2px solid black;
    width: 45%;
    padding: 15px;
    text-align: left;
}
&lt;/style&gt;
&lt;/head&gt;&lt;/p&gt;

&lt;p&gt;Accuracy Report:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;MultiNB&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;78.15%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Logistic&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;80.22%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;SVM&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;80.71%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;MultiNB&lt;/td&gt;
&lt;td&gt;Negation&lt;/td&gt;
&lt;td&gt;78.66%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;MultiNB&lt;/td&gt;
&lt;td&gt;stopwords&lt;/td&gt;
&lt;td&gt;76.68%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Logistic&lt;/td&gt;
&lt;td&gt;Ngram(1-3)&lt;/td&gt;
&lt;td&gt;82.08%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;SVM&lt;/td&gt;
&lt;td&gt;Ngram(1-3)&lt;/td&gt;
&lt;td&gt;84.62%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;SVM Report:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;F1 score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.83&lt;/td&gt;
&lt;td&gt;0.86&lt;/td&gt;
&lt;td&gt;0.85&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.86&lt;/td&gt;
&lt;td&gt;0.83&lt;/td&gt;
&lt;td&gt;0.84&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;ROC Curve:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../projects/sentiment/output_51_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From the tables above we can easily find that SVM and Logistic Regression are both better than Naive Bayes and they perform almost the same for our dataset. This is obviously true because the high bias low variance classifiers like Naive Bayes work well only in small dataset(&amp;lt; 10000).&lt;/p&gt;

&lt;p&gt;When introducing some feature engineering process such as emoji changing and negation, the accuracy remains almost the same and removing stop words even cause the accuracy decreasing by 2%. It tells us that stop words are informative especially in short text like tweets.&lt;/p&gt;

&lt;p&gt;Mixing unigram with bigram and trigram seems to a significant improvement for logistic(2%) and SVM(4%). Here we discard Navie Beyes since it is not proper for our data based on early discussions.&lt;/p&gt;

&lt;h2 id=&#34;summary-and-future&#34;&gt;Summary and Future&lt;/h2&gt;

&lt;p&gt;The best combination of algorithm and feature engineering method is SVM + Mixed Ngram. SVM with linear kernal is good both on binary text classification and computational performance while bag of single word analysis is not informative enough.&lt;/p&gt;

&lt;p&gt;In the future, we are going to perform multinomial classification such as positive, neutral and negative and also train different models for different topic.&lt;/p&gt;

&lt;h2 id=&#34;a-small-sample-of-code&#34;&gt;A Small Sample of Code&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
from nltk.corpus import stopwords
import nltk
import re
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Randomly select about 25% of original dataset and split it into training(80%) and test(20%).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;whole_data = pd.read_csv(&#39;Sentiment Analysis Dataset.csv&#39;, header=0, error_bad_lines=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;b&#39;Skipping line 8836: expected 4 fields, saw 5\n&#39;
b&#39;Skipping line 535882: expected 4 fields, saw 7\n&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;whole_data.shape
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(1578612, 4)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;idx = np.random.randint(0,1578612,400000)
reduce_data = whole_data.loc[idx]
data_train, data_test = train_test_split(reduce_data, test_size = 0.2)
train = data_train[[&#39;Sentiment&#39;, &#39;SentimentText&#39;]].reset_index().drop(&#39;index&#39;, axis=1)
test = data_test[[&#39;Sentiment&#39;, &#39;SentimentText&#39;]].reset_index().drop(&#39;index&#39;, axis=1)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;test.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Sentiment&lt;/th&gt;
      &lt;th&gt;SentimentText&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;One of my best friends gave birth 2 weeks ago ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;@Diana_Rosalien haha HELLO! i&#39;m home! and i go...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;@jamie_oliver had a nice weekend, didnt do muc...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;@Adman500 Haha, thankyou very much&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;@denifty I&#39;m really sorry. I thought updates w...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train[&#39;Sentiment&#39;].sum()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;160378
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train.to_csv(&#39;train.csv&#39;)
test.to_csv(&#39;test.csv&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Vectorization on each twiiter shorttext.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def feature_transform(sent):
    out = sent
    
    # feature set: emoji, hashtag, url(hyperlink), &amp;quot;I’m in a hurrryyyyy&amp;quot;
    feature_set = {r&#39;:-\)|:\)|\(:|\(-:&#39;: &#39;smile&#39;, \
                r&#39;:-D|:D|X-D|XD|xD&#39;: &#39;laugh&#39;, \
                r&#39;&amp;lt;3|:\*&#39;: &#39;kiss&#39;, \
                r&#39;;-\)|;\)|;-D|;D|\(;|\(-;&#39;: &#39;wink&#39;, \
                r&#39;:-\(|:\(|\(:|\(-:&#39;: &#39;frown&#39;, \
                r&#39;:,\(|:\&#39;\(|:&amp;quot;\(|:\(\(&#39;: &#39;cry&#39;,\
                r&#39;#(\w+)&#39;: r&#39;HASH_\1&#39;, \
                r&#39;(http|https|ftp)://[a-zA-Z0-9\\./]+&#39;: &#39;URL&#39;, \
                r&#39;(.)\1{1,}&#39;: r&#39;\1\1&#39;}
    
    for key, value in feature_set.items():
        #print(key, value)
        out = re.sub(key, value, out)
        
    return out
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;test.ix[4,&#39;SentimentText&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;@denifty I&#39;m really sorry. I thought updates were free for touch users too &amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MultiNB&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;test = pd.read_csv(&#39;test.csv&#39;, header = 0)
y_test = test.Sentiment
X_test = test.SentimentText
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train = pd.read_csv(&#39;train.csv&#39;, header = 0)
y = train.Sentiment
X = train.SentimentText
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text_clf = Pipeline([(&#39;vect&#39;, CountVectorizer()), \
                     (&#39;tfidf&#39;, TfidfTransformer()), \
                     (&#39;clf&#39;, MultinomialNB())])

text_clf.fit(X,y)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Pipeline(steps=[(&#39;vect&#39;, CountVectorizer(analyzer=&#39;word&#39;, binary=False, decode_error=&#39;strict&#39;,
        dtype=&amp;lt;class &#39;numpy.int64&#39;&amp;gt;, encoding=&#39;utf-8&#39;, input=&#39;content&#39;,
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip...inear_tf=False, use_idf=True)), (&#39;clf&#39;, MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pred = text_clf.predict(X_test)
acc_NB = np.mean(pred == y_test)
acc_NB
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.78158749999999999
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Logistic&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text_clf = Pipeline([(&#39;vect&#39;, CountVectorizer()), \
                     (&#39;tfidf&#39;, TfidfTransformer()), \
                     (&#39;clf&#39;, LogisticRegression())])

text_clf.fit(X,y)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Pipeline(steps=[(&#39;vect&#39;, CountVectorizer(analyzer=&#39;word&#39;, binary=False, decode_error=&#39;strict&#39;,
        dtype=&amp;lt;class &#39;numpy.int64&#39;&amp;gt;, encoding=&#39;utf-8&#39;, input=&#39;content&#39;,
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip...ty=&#39;l2&#39;, random_state=None, solver=&#39;liblinear&#39;, tol=0.0001,
          verbose=0, warm_start=False))])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pred = text_clf.predict(X_test)
logit_NB = np.mean(pred == y_test)
logit_NB
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.8021625
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;logistic with Ngram&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text_clf = Pipeline([(&#39;vect&#39;, CountVectorizer(ngram_range=(1,3))), \
                     (&#39;tfidf&#39;, TfidfTransformer()), \
                     (&#39;clf&#39;, LogisticRegression())])

text_clf.fit(X,y)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Pipeline(steps=[(&#39;vect&#39;, CountVectorizer(analyzer=&#39;word&#39;, binary=False, decode_error=&#39;strict&#39;,
        dtype=&amp;lt;class &#39;numpy.int64&#39;&amp;gt;, encoding=&#39;utf-8&#39;, input=&#39;content&#39;,
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 3), preprocessor=None, stop_words=None,
        strip...ty=&#39;l2&#39;, random_state=None, solver=&#39;liblinear&#39;, tol=0.0001,
          verbose=0, warm_start=False))])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pred = text_clf.predict(X_test)
logit_NB_ngram = np.mean(pred == y_test)
logit_NB_ngram
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.82082500000000003
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SVM&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text_clf = Pipeline([(&#39;vect&#39;, CountVectorizer(ngram_range=(1,3))), \
                     (&#39;tfidf&#39;, TfidfTransformer()), \
                     (&#39;clf&#39;, LinearSVC())])

text_clf.fit(X,y)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Pipeline(steps=[(&#39;vect&#39;, CountVectorizer(analyzer=&#39;word&#39;, binary=False, decode_error=&#39;strict&#39;,
        dtype=&amp;lt;class &#39;numpy.int64&#39;&amp;gt;, encoding=&#39;utf-8&#39;, input=&#39;content&#39;,
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 3), preprocessor=None, stop_words=None,
        strip...ax_iter=1000,
     multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=None, tol=0.0001,
     verbose=0))])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pred_svm = text_clf.predict(X_test)
svm_ngram = np.mean(pred == y_test)
svm_ngram
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.84617500000000001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hypeparameter tuning&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;parameters = {&#39;clf__C&#39;:(0.01,0.1,1,10)}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, cv=2)
gs_clf_result= gs_clf.fit(X[:100000],y[:100000])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 16.2 s, sys: 941 ms, total: 17.2 s
Wall time: 1min 12s
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time 
clf = text_clf
clf.fit(X, y)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 53.5 s, sys: 3.17 s, total: 56.6 s
Wall time: 57.7 s
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;gs_clf_result.best_score_
gs_clf_result.best_params_
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;{&#39;clf__C&#39;: 1}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SVM Classification report&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(classification_report(y_test,pred_svm))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;             precision    recall  f1-score   support

          0       0.83      0.86      0.85     39962
          1       0.86      0.83      0.84     40038

avg / total       0.85      0.85      0.85     80000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ROC Curve&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text_clf = Pipeline([(&#39;vect&#39;, CountVectorizer()), \
                     (&#39;tfidf&#39;, TfidfTransformer()), \
                     (&#39;clf&#39;, MultinomialNB())])

text_clf.fit(X,y)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Pipeline(steps=[(&#39;vect&#39;, CountVectorizer(analyzer=&#39;word&#39;, binary=False, decode_error=&#39;strict&#39;,
        dtype=&amp;lt;class &#39;numpy.int64&#39;&amp;gt;, encoding=&#39;utf-8&#39;, input=&#39;content&#39;,
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip...inear_tf=False, use_idf=True)), (&#39;clf&#39;, MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y_test_prob = text_clf.predict_proba(X_test)[:,1]
fpr_nb, tpr_nb, thresholds_nb =roc_curve(y_test,Y_test_prob)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text_clf = Pipeline([(&#39;vect&#39;, CountVectorizer()), \
                     (&#39;tfidf&#39;, TfidfTransformer()), \
                     (&#39;clf&#39;, LogisticRegression())])

text_clf.fit(X,y)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Pipeline(steps=[(&#39;vect&#39;, CountVectorizer(analyzer=&#39;word&#39;, binary=False, decode_error=&#39;strict&#39;,
        dtype=&amp;lt;class &#39;numpy.int64&#39;&amp;gt;, encoding=&#39;utf-8&#39;, input=&#39;content&#39;,
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip...ty=&#39;l2&#39;, random_state=None, solver=&#39;liblinear&#39;, tol=0.0001,
          verbose=0, warm_start=False))])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y_logit_prob = text_clf.predict_proba(X_test)[:,1]
fpr_logit, tpr_logit, thresholds_logit =roc_curve(y_test,Y_logit_prob)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text_clf = Pipeline([(&#39;vect&#39;, CountVectorizer(ngram_range=(1,3))), \
                     (&#39;tfidf&#39;, TfidfTransformer()), \
                     (&#39;clf&#39;, LogisticRegression())])

text_clf.fit(X,y)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Pipeline(steps=[(&#39;vect&#39;, CountVectorizer(analyzer=&#39;word&#39;, binary=False, decode_error=&#39;strict&#39;,
        dtype=&amp;lt;class &#39;numpy.int64&#39;&amp;gt;, encoding=&#39;utf-8&#39;, input=&#39;content&#39;,
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 3), preprocessor=None, stop_words=None,
        strip...ty=&#39;l2&#39;, random_state=None, solver=&#39;liblinear&#39;, tol=0.0001,
          verbose=0, warm_start=False))])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y_logit_ngram_prob = text_clf.predict_proba(X_test)[:,1]
fpr_logit_ngram, tpr_logit_ngram, thresholds_logit_ngram =roc_curve(y_test,Y_logit_ngram_prob)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.plot(fpr_nb, tpr_nb, label=&#39;NB&#39;)
plt.plot(fpr_logit, tpr_logit, label=&#39;logit&#39;)
plt.plot(fpr_logit_ngram, tpr_logit_ngram, label=&#39;logit_ngram&#39;)
plt.plot([0, 1], [0, 1], &#39;--&#39;, label=&#39;random decision&#39;)
plt.xlabel(&#39;Fail positive rate&#39;)
plt.ylabel(&#39;True positive rate&#39;)
plt.legend(loc=4)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;auc(fpr_logit,tpr_logit)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.86817554821593235
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>TensorFlow Self-Learning Note 1</title>
      <link>/blog/tensorflow/tensorflowone/</link>
      <pubDate>Fri, 31 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/tensorflow/tensorflowone/</guid>
      <description>&lt;p&gt;What is the best deep learning framework? There is no answer to this question but I am sure that TensorFlow is one of the most competitive candidates. Come and learn tensorflow from the very begining with me!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-a-data-flow-graph&#34;&gt;What is a Data Flow Graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#features-of-tensorFlow&#34;&gt;Features of TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#basis&#34;&gt;Basis&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#graph-computing&#34;&gt;Graph Computing&lt;/a&gt;&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#create-a-graph&#34;&gt;Create a Graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#run-a-session&#34;&gt;Run a Session&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#working-in-interactive-environment&#34;&gt;Working in Interactive Environment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variables&#34;&gt;Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fetches&#34;&gt;Fetches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feeds&#34;&gt;Feeds&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-of-linear-regression&#34;&gt;Example of Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mnist-handwritten-digits-recognition&#34;&gt;MNIST Handwritten Digits Recognition&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#model-training&#34;&gt;Model Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-evaluation&#34;&gt;Model Evaluation&lt;/a&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorflow&#34;&gt;TensorFlow&lt;/a&gt; is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) that flow between them. This flexible architecture lets you deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device without rewriting code. TensorFlow also includes TensorBoard, a data visualization toolkit.&lt;/p&gt;

&lt;p&gt;TensorFlow was originally developed by researchers and engineers working on the Google Brain team within Google&amp;rsquo;s Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research. The system is general enough to be applicable in a wide variety of other domains, as well.(cited from github)&lt;/p&gt;

&lt;h3 id=&#34;what-is-a-data-flow-graph&#34;&gt;What is a Data Flow Graph&lt;/h3&gt;

&lt;p&gt;A data-flow graph (DFG) is a graph which represents a data dependancies between a number of operations. The nodes in the graph represent operations and edges connect nodes together by passing tensors. To conclude, tensors flowing in the DFG is TensorFlow.&lt;/p&gt;

&lt;h3 id=&#34;features-of-tensorflow&#34;&gt;Features of TensorFlow&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Flexibility&lt;/p&gt;

&lt;p&gt;TensorFlow is not only a neural network framwork. You can do whatever you want with the power of TensorFlow if you are able to make the calculations in a data flow graph&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Portability&lt;/p&gt;

&lt;p&gt;TensorFlow is able to work on any devices with CPU or GPU. You can focus on your idea without concerning about hardware environment&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Program Language Support&lt;/p&gt;

&lt;p&gt;TensorFlow currently support Python and C++&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Efficiency&lt;/p&gt;

&lt;p&gt;TensorFlow makes full use of hardware resources and maximizes computing performance&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;basis&#34;&gt;Basis&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Make all calculations as data flow graph&lt;/li&gt;
&lt;li&gt;Work with the graph by running a &lt;strong&gt;Session&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Present data as &lt;strong&gt;tensors&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;Variables&lt;/strong&gt; to keep information of status&lt;/li&gt;
&lt;li&gt;Fill in data and obtain the result of any operations with &lt;strong&gt;feeds&lt;/strong&gt; and &lt;strong&gt;fetches&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each node in the graph is called an &lt;strong&gt;op&lt;/strong&gt;(short for operation). An ops uses 0 or more &lt;strong&gt;tensors&lt;/strong&gt; and generates 0 or more &lt;strong&gt;tensors&lt;/strong&gt; with some calculations. A &lt;strong&gt;tensor&lt;/strong&gt; is a multi-dimensional array. You can take a graph as a 4 dimensional array [batch, height, witdth, channels] where all elements in the array are floats.&lt;/p&gt;

&lt;p&gt;Session puts all ops in the graph to CPUs or GPUs, then execute them with methods and return tensors finally. Tensor in Python is a numpy ndarray objects while in C++ is tensorflow::Tensor&lt;/p&gt;

&lt;h3 id=&#34;graph-computing&#34;&gt;Graph Computing&lt;/h3&gt;

&lt;p&gt;You can take creating a graph as construct a neural network while executing the session as training the network.&lt;/p&gt;

&lt;h4 id=&#34;create-a-graph&#34;&gt;Create a Graph&lt;/h4&gt;

&lt;p&gt;In TensorFlow, &lt;strong&gt;Constant&lt;/strong&gt; is a kind of &lt;strong&gt;op&lt;/strong&gt; without any input but you can make it a as a input of other ops. TensorFlow module in Python has a default graph so that you can directly generate ops on it. More details about &lt;a href=&#34;https://www.tensorflow.org/api_guides/python/framework#Graph&#34;&gt;Graph Class&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
import numpy as np
from PIL import Image
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create a Constant op that produces a 1x2 matrix.  The op is
# added as a node to the default graph.
#
# The value returned by the constructor represents the output
# of the Constant op.
matrix1 = tf.constant([[3., 3.]])

# Create another Constant that produces a 2x1 matrix.
matrix2 = tf.constant([[2.],[2.]])

# Create a Matmul op that takes &#39;matrix1&#39; and &#39;matrix2&#39; as inputs.
# The returned value, &#39;product&#39;, represents the result of the matrix
# multiplication.
product = tf.matmul(matrix1, matrix2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now there are 3 ops in the default graph: 2 &lt;strong&gt;Constant( ) ops&lt;/strong&gt; and 1 &lt;strong&gt;matmul( ) op&lt;/strong&gt;. In order to get the result of the matrices multiplication, we need a session the run the computings.&lt;/p&gt;

&lt;h4 id=&#34;run-a-session&#34;&gt;Run a Session&lt;/h4&gt;

&lt;p&gt;Find more details at &lt;a href=&#34;https://www.tensorflow.org/api_guides/python/client#session-management&#34;&gt;Session Class&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Launch the default graph.
sess = tf.Session()

# To run the matmul op we call the session &#39;run()&#39; method, passing &#39;product&#39;
# which represents the output of the matmul op.  This indicates to the call
# that we want to get the output of the matmul op back.
#
# All inputs needed by the op are run automatically by the session.  They
# typically are run in parallel.
#
# The call &#39;run(product)&#39; thus causes the execution of threes ops in the
# graph: the two constants and matmul.
#
# The output of the op is returned in &#39;result&#39; as a numpy `ndarray` object.
result = sess.run(product)
print(result)

# Close the Session when we&#39;re done.
sess.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[[ 12.]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Do not forget to close the session in the end. However, you may want to use &lt;strong&gt;with&lt;/strong&gt; to avoid the close&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with tf.Session() as sess:
  result = sess.run([product])
  print(result)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[array([[ 12.]], dtype=float32)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The computings are all on CPUs and GPUs. TensorFlow will use the first GPU by default if you are using GPU for executing. If you would like to work on other GPUs:
with tf.Session() as sess:
  with tf.device(&amp;ldquo;/gpu:1&amp;rdquo;):
    matrix1 = tf.constant([[3., 3.]])
    matrix2 = tf.constant([[2.],[2.]])
    product = tf.matmul(matrix1, matrix2)
    &amp;hellip;
Names for Devices:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&amp;rdquo;/cpu:0&amp;rdquo;： Your CPU&lt;/li&gt;
&lt;li&gt;&amp;rdquo;/gpu:0&amp;rdquo;： Your first GPU&lt;/li&gt;
&lt;li&gt;&amp;rdquo;/gpu:1&amp;rdquo;： Your second GPU&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Documentation of &lt;a href=&#34;https://www.tensorflow.org/tutorials/using_gpu&#34;&gt;GPU&lt;/a&gt; using&lt;/p&gt;

&lt;h3 id=&#34;working-in-interactive-environment&#34;&gt;Working in Interactive Environment&lt;/h3&gt;

&lt;p&gt;We were using Session and Session.run() to execute the graph computing. However, you can use InteractiveSession class and methods like Tensor.eval(), Operation.run() in interactive environments such IPython and Jupyter Notebook.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Enter an interactive TensorFlow Session.
sess = tf.InteractiveSession()

x = tf.Variable([1.0, 2.0])
a = tf.constant([3.0, 3.0])

# Initialize &#39;x&#39; using the run() method of its initializer op.
x.initializer.run()

# Add an op to subtract &#39;a&#39; from &#39;x&#39;.  Run it and print the result
sub = tf.sub(x, a)
print(sub.eval())

# Close the Session when we&#39;re done.
sess.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[-2. -1.]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;variables&#34;&gt;Variables&lt;/h3&gt;

&lt;p&gt;Variables will keep their information of status while running a session. They are usually used to represent parameters which are need to be trained with iterations in an algorithm since they can memorize the status of each iteration. The variable in the following example take the role as a simple counter.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create a Variable, that will be initialized to the scalar value 0.
state = tf.Variable(0, name=&amp;quot;counter&amp;quot;)

# Create an Op to add one to `state`.

one = tf.constant(1)
new_value = tf.add(state, one)
update = tf.assign(state, new_value)

# Variables must be initialized by running an `init` Op after having 
# launched the graph.  We first have to add the `init` Op to the graph.
init_op = tf.global_variables_initializer()

# Launch the graph and run the ops.
with tf.Session() as sess:
    # run the init op
    sess.run(init_op)
    # Print the initial value of &#39;state&#39;
    print(sess.run(state))
    # Run the op that updates &#39;state&#39; and print &#39;state&#39;.
    for i in range(3):
        sess.run(update)
        print(sess.run(state))
    
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0
1
2
3
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;fetches&#34;&gt;Fetches&lt;/h3&gt;

&lt;p&gt;To fetch the result of an op, you can &lt;strong&gt;print&lt;/strong&gt; the status after executing &lt;strong&gt;sess.run()&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input1 = tf.constant(3.0)
input2 = tf.constant(2.0)
input3 = tf.constant(5.0)
intermd = tf.add(input3, input2)
mul = tf.mul(input1, intermd)

with tf.Session() as sess:
    result = sess.run([mul, intermd])
    print(result)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[21.0, 7.0]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;feeds&#34;&gt;Feeds&lt;/h3&gt;

&lt;p&gt;First create a placeholder, then feed it with data&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input1 = tf.placeholder(tf.float32)
input2 = tf.placeholder(tf.float32)
output = tf.mul(input1, input2)

with tf.Session() as sess:
  print(sess.run([output], feed_dict={input1:[7.], input2:[2.]}))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[array([ 14.], dtype=float32)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not feeding a placehold will throw out errors.&lt;/p&gt;

&lt;h2 id=&#34;example-of-linear-regression&#34;&gt;Example of Linear Regression&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Randomly generate 100 pairs of (x, y) where y = x * 0.1 + 0.3
x_data = np.random.rand(100).astype(&amp;quot;float32&amp;quot;)
y_data = x_data * 0.1 + 0.3

# Initialize w and b with variables
W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))
b = tf.Variable(tf.zeros([1]))
y = W * x_data + b    # here + and * has been overloaded for tf.add() and tf.mul()

# Least Square Error
loss = tf.reduce_mean(tf.square(y - y_data))
optimizer = tf.train.GradientDescentOptimizer(0.5)
train = optimizer.minimize(loss)

# Initialize the variables in tensorflow
init = tf.global_variables_initializer()

# Run the session
with tf.Session() as sess:
    sess.run(init)
    # monitor the changes of coefficients every 20 iterations
    for step in range(201):
        sess.run(train)
        if step % 20 == 0:
            print(step, sess.run(W), sess.run(b))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0 [-0.21695098] [ 0.66673416]
20 [-0.00646714] [ 0.35819602]
40 [ 0.07154677] [ 0.31555283]
60 [ 0.09239592] [ 0.30415648]
80 [ 0.09796781] [ 0.30111083]
100 [ 0.09945691] [ 0.30029687]
120 [ 0.09985484] [ 0.30007935]
140 [ 0.09996121] [ 0.3000212]
160 [ 0.09998964] [ 0.30000567]
180 [ 0.09999724] [ 0.30000153]
200 [ 0.09999929] [ 0.3000004]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;mnist-handwritten-digits-recognition&#34;&gt;MNIST Handwritten Digits Recognition&lt;/h2&gt;

&lt;p&gt;MNIST is a dataset of simple handwritten digits. You can get access to the data &lt;a href=&#34;http://yann.lecun.com/exdb/mnist/&#34;&gt;here&lt;/a&gt;. There are 55,000 observations in training set and 10,000 in test set. Each 28x28 picture is flatten to an array of length 784 and the response is from 0 to 9. However, MNIST is the sample data set of tensorFlow in Python. It is quite easy to get the data by two lines of code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(&amp;quot;MNIST_data/&amp;quot;, one_hot=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;try_x,try_y =mnist.train.next_batch(1)
Image.fromarray(np.uint8(try_x.reshape(28,28)*255)).resize((300,300),Image.ANTIALIAS)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../../blog/tensorflow/output_30_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Since this is only an entry level multiclass classification, I will not discuss the details about the algorithm here but focus on the implementation with TensorFlow.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = tf.placeholder(tf.float32, [None, 784])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here x is about a specific number but a placeholder. We use [None, 784] to represent the whole MNIST data set where None could be anything.&lt;/p&gt;

&lt;p&gt;We use variables to represent weights and bias which are changeable&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;W and b are initialized with 0. W is 784x10 since the response is one hot vector and the bias has length 10.&lt;/p&gt;

&lt;p&gt;Implement &lt;strong&gt;softmax regression&lt;/strong&gt; only needs one line of code&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = tf.nn.softmax(tf.matmul(x, W) + b)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;model-training&#34;&gt;Model Training&lt;/h3&gt;

&lt;p&gt;Loss function is one of the essential parts of a machine learning algorithm and it indicates the goodness of training. Here we use cross-entropy as our loss function. Cross-entropy comes from information theory but not is applied in many other fields. Its defination is:
$$H_y^\prime(y)=-\sum_iy_i^\prime\log(y_i)$$ where y is the predicted probability and \(y^\prime\) is the true values. You can find more detials about cross-entropy &lt;a href=&#34;http://colah.github.io/posts/2015-09-Visual-Information/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In order to implement cross-entropy, we set a placeholder to store the true responses.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y_ = tf.placeholder(tf.float32, [None, 10])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then define the cross-entropy. This is not restricted to only one picture. It can be the whole data set.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
#cross_entropy = -tf.reduce_sum(y_ * tf.log(y))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;TensorFlow automatically uses back propagation to modify the parameters for the purpose of minizing the loss. The last thing you need to set is choosing a algorithm for minizing the loss. Here we select gradient descent. TensorFlow also offers other &lt;a href=&#34;https://www.tensorflow.org/api_guides/python/train#Optimizers&#34;&gt;optimization methods&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Initialize the variables before training
init = tf.global_variables_initializer()
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;model-evaluation&#34;&gt;Model Evaluation&lt;/h4&gt;

&lt;p&gt;We use &lt;strong&gt;tf.argmax()&lt;/strong&gt; to predict the label and &lt;strong&gt;tf.equal()&lt;/strong&gt; to find whether the predictions matche the true labels. correct_prediction is a list of boolean values. For example [True, False, True, True]. One can use &lt;strong&gt;tf.cast()&lt;/strong&gt; to convert it into [1, 0, 1, 1] and the accuracy is 0.75&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Start a session
sess = tf.Session()
# Initialize the variables
sess.run(init) 
# Train the model with 1000 as the maximum iterations
for i in range(1000):
    batch_xs, batch_ys = mnist.train.next_batch(10)
    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

# Model Evaluation.
correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

print(&amp;quot;Accuarcy on Test-dataset: &amp;quot;, sess.run(accuracy, \
                    feed_dict={x: mnist.test.images, y_: mnist.test.labels}))

sess.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Accuarcy on Test-dataset:  0.8688
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Softmax regression is too simple to get an ideal result. Here our result is around 0.88 while it can reach 0.97 by some simple optimization. Nowadays, the best accuracy is 99.7%.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>SPAM FILTER</title>
      <link>/projects/spam_detection/</link>
      <pubDate>Wed, 01 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/projects/spam_detection/</guid>
      <description>&lt;p&gt;You got a spam email! Gmail can always help you filtering spam emails. Do you want to know how it works?
&lt;/p&gt;

&lt;h3&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Email detection is one of the most importand applications of Natural Language Processing. The common ways are Hand-coded rules which is quite complicated in processing with low recall rate and supervised learning which includes Naïve Bayes、Logistic regression、Support-vector machines、k-Nearest Neighbors, etc. Here I will use the Naïve Bayes Classifier since it is more straightforward and simpler than other algorithms.&lt;!--more--&gt;&lt;/p&gt;

&lt;h3&gt;Algorithm Details&lt;/h3&gt;
Naïve Bayes is based on bayes rule and bag of words analysis with a very important assumption: tokens appeard in the text are disorderly and independant with each other. The Naïve Bayes Classifier can be explained like this:
$$P(c|d) = \frac{P(d|c)P(c)}{P(d)}$$ where d represents document and c represents classes. The class c of the given document d is:
$$
\begin{split}
C &amp;= argmax\ P(c|d) \\
&amp;=argmax\ \frac{P(d|c)P(c)}{P(d)}\\
&amp;=argmax\ P(d|c)P(c)\\
&amp;=argmax\ P(t_1,t_2,...,t_n|c)P(c)\\
&amp;=argmax\ P(t_1|c)P(t_2|c),...,P(t_n|c)P(c) \\
&amp;=argmax\ logP(t_1|c)+logP(t_2|c)+,...,+logP(t_n|c)+logP(c)
\end{split}
$$
where \(t_1,...,t_n\) are tokens in the given text. \(P(c)\) is the prior probability, \(P(t_1,t_2,...,t_n|c\) is the conditional probability, namely likelihood and P(c|d) is the posterior probability.



&lt;h3&gt;Training Phase&lt;/h3&gt;

&lt;p&gt;Our training data are emails in the SPAM_trainig folder with the file name like &amp;lsquo;HAM.02281.txt&amp;rsquo; representing ham email and &amp;lsquo;SPAM.02281.txt&amp;rsquo; representing spam email.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import nltk
import pickle
import math
import re
from collections import Counter
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;1. Load our training files and take a look at the contents&lt;/h4&gt;

&lt;p&gt;ham stores all true emails with the file name like HAM.0123.txt while spam stores all spam emails with the file name like SPAM.0234.txt&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ham = nltk.corpus.PlaintextCorpusReader(&#39;/Users/Aslan/winter 2017/Lin127/hw2,3/SPAM_training&#39;, &#39;HAM.*.txt&#39;)
spam = nltk.corpus.PlaintextCorpusReader(&#39;/Users/Aslan/winter 2017/Lin127/hw2,3/SPAM_training&#39;, &#39;SPAM.*.txt&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are 13545 ham emails and 4912 spam emails in our training dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# number of files in each class
ham_count = len(ham.fileids())
spam_count = len(spam.fileids())
# total number of tokens in each class
ham_tokens = ham.words()
spam_tokens = spam.words()
print(ham_count, spam_count)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;13545 4912
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;2. Build our text classifier&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_freq_df(words):
    &#39;&#39;&#39;Build a nltk frequency table for the token types in each class
    
    Args:
        words(nltk object): a list of tokens
    
    Returns:
        dist_words(nltk FreqDist): frequency table
    &#39;&#39;&#39;
    dist_words = nltk.FreqDist(words)
    return dist_words
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def solver(spam, ham, spam_count, ham_count):
    &#39;&#39;&#39;build the solver model
    Args: 
        spam(nltk object): a list of tokens in spam
        ham(nltk object): a list of tokens in ham
    Returns:
        a dict with two data frames and two counts for each class 
    &#39;&#39;&#39;
    df_spam = get_freq_df(spam)
    df_ham = get_freq_df(ham)

    return {&#39;spam_fd&#39;: df_spam, &#39;ham_fd&#39;: df_ham, 
            &#39;spam_count&#39;: spam_count, &#39;ham_count&#39;: ham_count}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3. Save our model into spam.nb for time saving when reusing this code.&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;solver_NB = solver(spam_tokens, ham_tokens, spam_count, ham_count)
with open(&#39;spam.nb&#39;, &#39;wb&#39;) as f:
    pickle.dump(solver_NB, f, protocol=pickle.HIGHEST_PROTOCOL)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Testing Phase&lt;/h3&gt;

&lt;h4&gt;1. we reload our classifier model from our hard disk&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with open(&#39;spam.nb&#39;, &#39;rb&#39;) as f:
    model = pickle.load(f)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;log_prior_spam/ham represents the prior logrithm probability of spam/ham class.
we can see that there are, in total,  1284301 tokens in spam training data and 4903935 in ham data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;log_prior_spam = math.log(model[&#39;spam_count&#39;]/(model[&#39;spam_count&#39;] + model[&#39;ham_count&#39;]))
log_prior_ham = math.log(model[&#39;ham_count&#39;]/(model[&#39;spam_count&#39;] + model[&#39;ham_count&#39;]))
spam_n = model[&#39;spam_fd&#39;].N()
ham_n = model[&#39;ham_fd&#39;].N()
print(spam_n, ham_n)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;1284301 4903935
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;2. create the likelihood table with add-one-smoothing modification&lt;/h4&gt;

&lt;p&gt;The add-one-smoothing is one of the methods to fix the new word problem. When there are new words which can be found only from one of our training class(spam or ham) in the text files, the posterior probablity could be zero since \(P(new|c) = 0\) for the class that does not include that new word. We don&amp;rsquo;t want our classifier be disturbed by these new words and that is why we should implement this modification here.&lt;/p&gt;

&lt;p&gt;To be more detailed:
$$P(token|c) = \frac{(number\ of\ the\ given\ token\ in\ class\ c) +1}{(token\ size\ of\ class\ c) + (vocabulary\ size)}$$ where the vocabulary size is the total number of token type in the union of spam and ham training set&lt;/p&gt;

&lt;p&gt;However, add-one-smoothing can&amp;rsquo;t fix the scenario where the new word doesn&amp;rsquo;t show in any traning class. In that case, I just simply omit that word.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Build the vacabulary set on the union of spam and ham training set.
vocabulary = set(list(model[&#39;ham_fd&#39;].keys()) + list(model[&#39;spam_fd&#39;].keys()))
vocal_size = len(vocabulary)
vocal_size
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;101357
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;spam_log_likelihood = {}    # initialize the conditional probablity
ham_log_likelihood = {}
# add-one-smoothing
for token in vocabulary:
    spam_log_likelihood[token] = math.log((model[&#39;spam_fd&#39;][token] + 1) / (spam_n + vocal_size))
    ham_log_likelihood[token] = math.log((model[&#39;ham_fd&#39;][token] + 1) / (ham_n + vocal_size))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def spam_detection(file, ham_log_likelihood, spam_log_likelihood, 
                   log_prior_spam, log_prior_ham, vocabulary):
    &#39;&#39;&#39; classify the given file as a binary outcome(spam or ham)
    Args:
        file: file name(eg.&#39;test.0123.txt&#39;)
        ham(spam)_log_likelihood: likelihood table after taking add-one-smoothing and logrithm
        log_prior_spam(ham): prior probability in log type
        vocabulary: token types in the union of spam and ham training dataset.
    Returns:
        print file name with &#39;SPAM&#39; or &#39;HAM&#39;
    &#39;&#39;&#39;       
    token_ls = dev.words(fileids=file)
    log_posterior_spam = log_prior_spam
    log_posterior_ham = log_prior_ham
    
    for token in token_ls:
        if not token in vocabulary:
            continue
        log_posterior_spam += spam_log_likelihood[token]
        log_posterior_ham += ham_log_likelihood[token]
    
    if log_posterior_spam &amp;gt; log_posterior_ham:
        print(file, &#39;SPAM&#39;)
    else:
        print(file, &#39;HAM&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;3. Run the classifier on our test data&lt;/h4&gt;

&lt;p&gt;I wrote the code above in a more decent way in a script file called nbtest.py and run the shell command:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;python3 nbtest.py &amp;gt; SPAM_dev_predictions.txt&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Then we load the result into variable dev by nltk&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dev = nltk.corpus.PlaintextCorpusReader(&#39;/Users/Aslan/winter 2017/Lin127/hw2,3/&#39;, &#39;SPAM_dev_predictions.txt&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Model Evaluation Phase&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;file_ls = dev.raw().strip().split(&#39;\n&#39;)
confusion = [re.search(r&#39;(HAM|SPAM)\..*(HAM|SPAM)&#39;, i).group(1) + &#39; &#39; +
             re.search(r&#39;(HAM|SPAM)\..*(HAM|SPAM)&#39;, i).group(2)
             for i in file_ls]
file_ls[:10]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[&#39;HAM.00039.txt HAM&#39;,
 &#39;HAM.00045.txt HAM&#39;,
 &#39;HAM.00064.txt HAM&#39;,
 &#39;HAM.00091.txt HAM&#39;,
 &#39;HAM.00098.txt HAM&#39;,
 &#39;HAM.00137.txt HAM&#39;,
 &#39;HAM.00143.txt HAM&#39;,
 &#39;HAM.00146.txt HAM&#39;,
 &#39;HAM.00153.txt HAM&#39;,
 &#39;HAM.00205.txt HAM&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Counter(confusion)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Counter({&#39;HAM HAM&#39;: 986, &#39;HAM SPAM&#39;: 14, &#39;SPAM HAM&#39;: 14, &#39;SPAM SPAM&#39;: 349})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our test set has 1000 ham and 363 spam
Therefore:&lt;/p&gt;

&lt;p&gt;\(recall(HAM) = \frac{986}{1000} = 98.6\% \)&lt;/p&gt;

&lt;p&gt;\(recall(SPAM) = \frac{349}{363} = 96.1\% \)&lt;/p&gt;

&lt;p&gt;\(precision(HAM) = \frac{986}{986+14} = 98.6\% \)&lt;/p&gt;

&lt;p&gt;\(precision(SPAM) = \frac{349}{349+14} = 96.1\% \)&lt;/p&gt;

&lt;p&gt;where \(t_1,&amp;hellip;,t_n\) are tokens in the given text. P(c) is the prior probability, \(P(t_1,t_2,&amp;hellip;,t_n|c\) is the conditional probability, namely likelihood and P(c|d) is the posterior probability.&lt;/p&gt;

&lt;p&gt;The recall and precision for both class are pretty high which indicate that my classifier is powerful enough.&lt;/p&gt;

&lt;h3&gt;Next Steps to be done&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Try other smoothing methods&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Find a way to deal with the new words problem which I omitted this time&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
  </channel>
</rss>